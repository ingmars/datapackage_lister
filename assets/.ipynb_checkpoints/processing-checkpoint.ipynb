{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fdd7071a-0bcf-48c5-9ed7-772ab69e59df"
    }
   },
   "source": [
    "<table style=\"width:100%; background-color: #EBF5FB\">\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid #CFCFCF\">\n",
    "      <b>Time series: Processing Notebook</b>\n",
    "      <ul>\n",
    "        <li><a href=\"main.ipynb\">Main Notebook</a></li>\n",
    "        <li>Processing Notebook</li>\n",
    "      </ul>\n",
    "      <br>This Notebook is part of the <a href=\"http://data.open-power-system-data.org/time_series\">Time series Data Package</a> of <a href=\"http://open-power-system-data.org\">Open Power System Data</a>.\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7c736530-7e78-4477-90ad-9b8230ce0cba"
    }
   },
   "source": [
    "# Table of Contents\n",
    "* [1. Introductory Notes](#1.-Introductory-Notes)\n",
    "* [2. Settings](#2.-Settings)\n",
    "\t* [2.1 Import Python libraries](#2.1-Import-Python-libraries)\n",
    "\t* [2.2 Select scope](#2.2-Select-scope)\n",
    "\t* [2.3 Select subset](#2.3-Select-subset)\n",
    "* [3. Download](#3.-Download)\n",
    "\t* [3.1 Automatic download (for all sources except Energinet.dk)](#3.1-Automatic-download-%28for-all-sources-except-Energinet.dk%29)\n",
    "\t* [3.2 Manual download (Energinet.dk)](#3.2-Manual-download-%28Energinet.dk%29)\n",
    "* [4. Read](#4.-Read)\n",
    "* [5. Processing](#5.-Processing)\n",
    "\t* [5.1 Missing Data Handling](#5.1-Missing-Data-Handling)\n",
    "\t* [5.2 Country specific calculations](#5.2-Country-specific-calculations)\n",
    "\t\t* [5.2.1 Calculate onshore wind generation for German TSOs](#5.2.1-Calculate-onshore-wind-generation-for-German-TSOs)\n",
    "\t\t* [5.2.2 Calculate aggregate wind capacity for germany (on + offshore)](#5.2.2-Calculate-aggregate-wind-capacity-for-germany-%28on-+-offshore%29)\n",
    "\t\t* [5.2.3 Aggregate German data from individual TSOs and calculate availabilities/profiles](#5.2.3-Aggregate-German-data-from-individual-TSOs-and-calculate-availabilities/profiles)\n",
    "\t* [5.3 Reorder Levels](#5.3-Reorder-Levels)\n",
    "\t* [5.4 Create hourly data from 15' data](#5.4-Create-hourly-data-from-15'-data)\n",
    "\t* [5.5 Insert a column with Central European (Summer-)time](#5.5-Insert-a-column-with-Central-European-%28Summer-%29time)\n",
    "\t* [5.6 Reorder columns](#5.6-Reorder-columns)\n",
    "* [6. Create metadata](#6.-Create-metadata)\n",
    "* [7. Write data to disk](#7.-Write-data-to-disk)\n",
    "\t* [7.1 Different shapes](#7.1-Different-shapes)\n",
    "\t* [7.2 Write to SQL-database](#7.2-Write-to-SQL-database)\n",
    "\t* [7.3 Write to Excel](#7.3-Write-to-Excel)\n",
    "\t* [7.4 Write to CSV](#7.4-Write-to-CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bba7a58e-f3b2-4d3b-9617-d734c369084f"
    }
   },
   "source": [
    "# 1. Introductory Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook handles missing data, performs calculations and aggragations and creates the output files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ba6b62da-6cee-476b-a563-c945f3fd0f79"
    }
   },
   "source": [
    "# 2. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b838df4-f987-4ae4-a132-9c898e3ffab1"
    }
   },
   "source": [
    "## 2.1 Import Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d4e82608-1ad1-4c51-8285-929a5a92c5b6"
    }
   },
   "source": [
    "This section: load libraries and set up a log.\n",
    "\n",
    "Note that the download module makes use of the [pycountry](https://pypi.python.org/pypi/pycountry) library that is not part of Anaconda. Install it with with `pip install pycountry` from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c0035fc6-ff1d-44d8-a3fd-b4c08f53be71"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import sqlite3\n",
    "import yaml\n",
    "import itertools\n",
    "import os\n",
    "import pytz\n",
    "\n",
    "from timeseries_scripts.read import read\n",
    "from timeseries_scripts.download import download\n",
    "from timeseries_scripts.imputation import find_nan\n",
    "from timeseries_scripts.make_json import make_json\n",
    "\n",
    "# reload modules with execution of any code, to avoid having to restart\n",
    "# the kernel after editing timeseries_scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sources_yaml_path = 'input/sources.yml'\n",
    "out_path = 'original_data'\n",
    "\n",
    "logger = logging.getLogger('log')\n",
    "# For more detailed logging messages, replace 'INFO' with 'DEBUG'\n",
    "# (May slow down computation).\n",
    "logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8ed60fdb-284d-43db-b802-8e5c405b8e84"
    }
   },
   "source": [
    "## 2.2 Select scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section: select the time range and the data sources for download and read. Default: all data sources implemented, full time range available.\n",
    "\n",
    "**Source parameters** are specified in [input/sources.yml](input/sources.yml), which describes, for each source, the datasets (such as wind and solar generation) alongside all the parameters necessary to execute the downloads.\n",
    "\n",
    "The option to perform downloading and reading is for testing only. To be able to run the script succesfully until the end, all sources have to be included, or otherwise the script will run into errors (i.e. the step where aggregate German timeseries are caculated requires data from all four German TSOs to be loaded).\n",
    "\n",
    "Instead of downloading from the sources, the complete raw data can be downloaded as a zip file from the OPSD Server. Advantages are:\n",
    "- much faster download\n",
    "- back up of raw data in case it is deleted from the server at the source original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f3008d47-ec89-40d0-85ee-776d110f3bb4"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the beginning and end of the interval for which to attempt\n",
    "# the download. Type None to dowwnload all available data.\n",
    "start_from_user = date(2016, 1, 1)  # i.e. date(2016, 1, 1)\n",
    "end_from_user = date(2016, 9, 30)  # i.e. date(2016, 1, 31)\n",
    "\n",
    "# Specify an archive version to use the raw data from that version that has\n",
    "# been cached on the OPSD server as input.\n",
    "# All data from that version will be downloaded - subset will be ignored.\n",
    "# Type None to download directly from the original sources.\n",
    "archive_version = None  # i.e. '2016-07-14'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Select subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, specify a subset to download/read.<br>\n",
    "The next cell prints the available sources and datasets.<br>\n",
    "Copy from its output and paste to following cell to get the right format.<br>\n",
    "Type `subset = None` to include all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSE:\n",
      "- wind\n",
      "\n",
      "Svenska Kraftnaet:\n",
      "- wind_solar_2\n",
      "- wind_solar_3\n",
      "- wind_solar_5\n",
      "- wind_solar_4\n",
      "- wind_solar_7\n",
      "- wind_solar_6\n",
      "- wind_solar_1\n",
      "\n",
      "ENTSO-E Data Portal:\n",
      "- load\n",
      "\n",
      "50Hertz:\n",
      "- solar_generation\n",
      "- wind_forecast_pre-offshore\n",
      "- wind_generation_pre-offshore\n",
      "- wind_generation_with-offshore\n",
      "- wind_forecast_with-offshore\n",
      "- solar_forecast\n",
      "\n",
      "TenneT:\n",
      "- wind\n",
      "- solar\n",
      "\n",
      "TransnetBW:\n",
      "- wind\n",
      "- solar\n",
      "\n",
      "OPSD:\n",
      "- capacities\n",
      "\n",
      "Elia:\n",
      "- wind-onshore\n",
      "- solar\n",
      "- wind-offshore\n",
      "\n",
      "Amprion:\n",
      "- wind\n",
      "- solar\n",
      "\n",
      "CEPS:\n",
      "- wind_pv\n",
      "\n",
      "Energinet.dk:\n",
      "- prices_wind_solar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(sources_yaml_path, 'r') as f:\n",
    "    sources = yaml.load(f.read())\n",
    "for k, v in sources.items():\n",
    "    print(yaml.dump({k: list(v.keys())}, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = yaml.load('''\n",
    "insert_source_here:\n",
    "- insert_dataset1_from_that_source_here\n",
    "- insert_dataset2_here\n",
    "more_sources:\n",
    "- more_data_sets\n",
    "''')  # Or\n",
    "subset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = yaml.load('''\n",
    "PSE:\n",
    "- wind\n",
    "''')  # Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read source parameters, eliminating sources and variables not in subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(sources_yaml_path, 'r') as f:\n",
    "    sources = yaml.load(f.read())\n",
    "if subset:  # eliminate sources and variables not in subset\n",
    "    sources = {source_name: {k: v\n",
    "                             for k, v in sources[source_name].items()\n",
    "                             if k in variable_list}\n",
    "               for source_name, variable_list in subset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section: download data. Takes about 1 hour to run for the complete data set (`subset=None`).\n",
    "\n",
    "First, a data directory is created on your local computer. Then, download parameters for each data source are defined, including the URL. These parameters are then turned into a YAML-string. Finally, the download is executed file by file.\n",
    "\n",
    "Each file is saved under it's original filename. Note that the original file names are often not self-explanatory (called \"data\" or \"January\"). The files content is revealed by its place in the directory structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e08368cb-021f-453a-b201-a8d48bc8e4c4"
    }
   },
   "source": [
    "## 3.1 Automatic download (for all sources except Energinet.dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0c1eb987-6d5f-4e3d-9248-df80b9f37a49"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logger.setLevel('INFO')\n",
    "download(sources, out_path,\n",
    "         archive_version=archive_version,\n",
    "         start_from_user=start_from_user,\n",
    "         end_from_user=end_from_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bae7810a-d4af-4021-a9ab-c4b772a2bd53"
    }
   },
   "source": [
    "## 3.2 Manual download (Energinet.dk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a69f8271-e1fc-4e64-814c-4210fb80c006"
    }
   },
   "source": [
    "Energinet.dk data needs to be downloaded manually from http://www.energinet.dk/en/el/engrosmarked/udtraek-af-markedsdata/Sider/default.aspx<br>\n",
    "Check The Boxes as specified below, then press the \"Get extract\"-button at the end of the page.\n",
    "\n",
    "You will receive a file `Market Data.xls` of about 50 MB. Open the file in Excel. There will be a warning from Excel saying that file extension and content are in conflict. Select \"open anyways\" and and save the file as `.xlsx`. This will compress the file by ~80%, resulting in faster processing afterwards. \n",
    "\n",
    "In order to be found by the read-function, place the downloaded file in the following subdirectory:<br>\n",
    "` \\original_data\\Energinet.dk\\prices_wind_solar\\2000-01-01_2016-09-30\\ ` <br>\n",
    "\n",
    "**Boxes to check:**\n",
    "\n",
    "Period<br>\n",
    "Get data from: 01-01-2000 To: Today<br>\n",
    "all months\n",
    "\n",
    "Data columns <br>\n",
    "Elspot Price, Currency Code/MWh\n",
    "\n",
    "- DK-West\n",
    "- DK-East\n",
    "- Norway\n",
    "- Sweden (SE)\n",
    "- Sweden (SE3)\n",
    "- Sweden (SE4)\n",
    "- DE European Power Exchange\n",
    "\n",
    "Production and consumption, MWh/h\n",
    "\n",
    "- DK-West: Wind power production\n",
    "- DK-West: Solar cell production (estimated)\n",
    "- DK-East: Wind power production\n",
    "- DK-East: Solar cell production (estimated)\n",
    "- DK: Wind power production (onshore)\n",
    "- DK: Wind power production (offshore)\n",
    "\n",
    "Data format:<br>\n",
    "Currency code\n",
    "- EUR\n",
    "\n",
    "Decimal format\n",
    "- English number Format (period as decimal separator)\n",
    "\n",
    "Date format\n",
    "- Other date format(YYYY-MM-DD)\n",
    "\n",
    "Recieve to\n",
    "- Excel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d0b353e7-179d-4556-bdd2-270192c830fb"
    }
   },
   "source": [
    "# 4. Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d223207e-2b0e-49ec-8bf6-af2969ee5b28"
    }
   },
   "source": [
    "This section: Read each downloaded file into a pandas-DataFrame and merge data from different sources if it has the same time resolution. Takes ~15 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1300cbb2-efde-4844-b08c-fed092023e38"
    }
   },
   "source": [
    "Set the title of the rows at the top of the data used to store metadata internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4dc92cc3-c01d-4c83-9252-80958edbe0f9"
    }
   },
   "outputs": [],
   "source": [
    "headers = ['variable', 'region', 'attribute', 'source', 'web']\n",
    "headers = ['region', 'variable', 'attribute', 'source', 'web']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of empty DataFrames to be populated by the rea function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = {'15min': pd.DataFrame(), '60min': pd.DataFrame()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through sources and variables to do the reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:log:reading PSE - wind\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [#########-----------------------------------------] 274/1589 files Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logger.setLevel('INFO')\n",
    "# For each source in the source dictionary\n",
    "for source_name, source_dict in sources.items():\n",
    "    # For each variable from source_name\n",
    "    for variable_name, param_dict in source_dict.items():\n",
    "        variable_dir = os.path.join(out_path, source_name, variable_name)\n",
    "        res_key = param_dict['resolution']\n",
    "        url = param_dict['web']\n",
    "        df = read(source_name, variable_name, url, res_key, headers,\n",
    "                  out_path='original_data',\n",
    "                  start_from_user=start_from_user,\n",
    "                  end_from_user=end_from_user)\n",
    "\n",
    "        if data_sets[res_key].empty:\n",
    "            data_sets[res_key] = df\n",
    "        elif not df.empty:\n",
    "            data_sets[res_key] = (\n",
    "                data_sets[res_key]\n",
    "            ).combine_first(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "be24bc5f-e281-43bf-853a-e7130d20d2e8"
    }
   },
   "source": [
    "Display some rows of the dataframes to get a first impression of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min']['2015-07-01 12:00':].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "28b0b2ec-c15f-4baf-97ab-1993ec8e896f"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th>PL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attribute</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>PSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web</th>\n",
       "      <th>http://www.pse.pl/index.php?modul=21&amp;id_rap=24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-31 23:00:00</th>\n",
       "      <td>1365.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>1278.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>1199.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>1025.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>877.988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "region                                                          PL\n",
       "variable                                                      wind\n",
       "attribute                                               generation\n",
       "source                                                         PSE\n",
       "web                 http://www.pse.pl/index.php?modul=21&id_rap=24\n",
       "2015-12-31 23:00:00                                       1365.050\n",
       "2016-01-01 00:00:00                                       1278.125\n",
       "2016-01-01 01:00:00                                       1199.138\n",
       "2016-01-01 02:00:00                                       1025.988\n",
       "2016-01-01 03:00:00                                        877.988"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets['60min']['2015-07-01 12:00':].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "019c8ae9-8934-4074-adfd-278acd212152"
    }
   },
   "source": [
    "Save the DataFrames created by the read function to disk. This way you have the raw data to fall back to if something goes wrong in the ramainder of this notebook without having to repeat the previos steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min'].to_pickle('raw_15.pickle')\n",
    "data_sets['60min'].to_pickle('raw_60.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the DataFrames saved above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "data_sets['15min'] = pd.read_pickle('raw_15.pickle')\n",
    "data_sets['60min'] = pd.read_pickle('raw_60.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "026c6ad8-0422-4887-9361-8e45ae33e0c6"
    }
   },
   "source": [
    "# 5. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c446157a-7872-491b-a9c4-0336b87568a6"
    }
   },
   "source": [
    "This section: missing data handling, aggregation of sub-national to national data, aggregate 15'-data to 60'-resolution. Takes 30 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0c13e5eb-d8e3-425d-982e-2a25c364cdc5"
    }
   },
   "source": [
    "## 5.1 Missing Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "81d4d772-d3c7-4e07-a0a7-49afb3bdf8f6"
    }
   },
   "source": [
    "Patch missing data. At this stage, only small gaps (up to 2 hours) are filled by linear interpolation. This catched most of the missing data due to daylight savings time transitions, while leaving bigger gaps untouched\n",
    "\n",
    "The exact locations of missing data are stored in the `nan_table` DataFrames.\n",
    "\n",
    "Where data has been interpolated, it is marked in a new column `comment`. For eaxample the comment `solar_DE-transnetbw_generation;` means that in the original data, there is a gap in the solar generation timeseries from TransnetBW in the time period where the marker appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "533cd53f-eb8f-4699-bbf7-72ad1cf90325"
    }
   },
   "source": [
    "Patch the datasets and display the location of missing Data in the original data. Takes ~30 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "33602355-28cb-4d5c-b97e-fe59b9b48883"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time data_sets['15min'], nan_table15 = find_nan(data_sets['15min'], headers, patch=True)\n",
    "%time data_sets['60min'], nan_table60 = find_nan(data_sets['60min'], headers, patch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the patched data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min'].to_pickle('patched_15.pickle')\n",
    "data_sets['60min'].to_pickle('patched_60.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "data_sets['15min'] = pd.read_pickle('patched_15.pickle')\n",
    "data_sets['60min'] = pd.read_pickle('patched_60.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c5c4893-5098-47df-8e68-51b13f5fd484"
    }
   },
   "source": [
    "Display the table of regions of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b5207ca6-4a72-40b6-8afd-3123f66ae323"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nan_table15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nan_table60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export the NaN-tables to Excel in order to inspect where there are NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('NaN_table.xlsx')\n",
    "nan_table15.to_excel(writer, '15min')\n",
    "nan_table60.to_excel(writer, '60min')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "345c31ec-fa0b-4859-a930-8b7c20f1e1bf"
    }
   },
   "source": [
    "Execute this to see an example of where the data has been patched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min'].loc[data_sets['15min']['comment'].notnull(),'wind'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sets['60min'][data_sets['60min']['comment'].notnull()].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "abd61a95-68e4-4bf2-9f1f-ef010b4cd288"
    }
   },
   "source": [
    "## 5.2 Country specific calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0a2cc638-d6f5-4f7f-a044-fb61d20cf4ad"
    }
   },
   "source": [
    "### 5.2.1 Calculate onshore wind generation for German TSOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 50 Hertz, it is already in the data.\n",
    "For TenneT, it is calculated by substracting offshore from total generation.\n",
    "For Amprion and TransnetBW, onshore wind generation is just total wind generation.\n",
    "Takes <1 second to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some of the following operations require the Dataframes to be lexsorted in\n",
    "# the columns\n",
    "for res_key, df in data_sets.items():\n",
    "    df.sortlevel(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tso in ['DE-amprion', 'DE-tennet', 'DE-transnetbw']:\n",
    "    new_col_header = ('wind-onshore', tso, 'generation',\n",
    "                      'own calculation', 'own calculation')\n",
    "\n",
    "    if tso == 'DE-tennet':\n",
    "        offshore = data_sets['15min'].loc[:,('wind-offshore', 'DE-tennet', 'generation')]\n",
    "    else:\n",
    "        offshore = 0\n",
    "        \n",
    "    data_sets['15min'][new_col_header] = (\n",
    "        data_sets['15min'][('wind', tso, 'generation')] - offshore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Calculate aggregate wind capacity for germany (on + offshore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from being interesting on it's own, this is also required to calculate an aggregated wind-profile for Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_col_header = (\n",
    "    'wind', 'DE', 'capacity', 'BNetzA and Netztransparenz.de',\n",
    "    'http://data.open-power-system-data.org/renewable_power_plants')\n",
    "data_sets['15min'][new_col_header] = data_sets['15min'].loc(axis=1)[\n",
    "    (['wind-onshore', 'wind-offshore'], 'DE', 'capacity')].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cbcd1b7c-6c9c-4d1a-ab70-28276b421805"
    }
   },
   "source": [
    "### 5.2.3 Aggregate German data from individual TSOs and calculate availabilities/profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9ab0116d-402b-444b-acf2-925388a9886b"
    }
   },
   "source": [
    "The wind and solar in-feed data for the 4 German balancing areas is summed up and stored in in new columns, which are then used to calculate profiles, that is, the share of wind/solar capacity producing at a given time. The column headers are created in the fashion introduced in the read script. Takes 5 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "26a83b1c-682d-43ec-bb70-12a096089508"
    }
   },
   "outputs": [],
   "source": [
    "for tech in ['solar', 'wind', 'wind-onshore', 'wind-offshore']:\n",
    "    for attribute in ['generation']:  # we could also include 'forecast'\n",
    "        sum_col = pd.Series()\n",
    "        for tso in ['DE-50hertz', 'DE-amprion', 'DE-tennet', 'DE-transnetbw']:\n",
    "            try:\n",
    "                add_col = data_sets['15min'][tech, tso, attribute]\n",
    "                if len(sum_col) == 0:\n",
    "                    sum_col = add_col\n",
    "                else:\n",
    "                    sum_col = sum_col + add_col.values\n",
    "            except KeyError:  # Occurs i.e. for Amprion and TransnetBW, which\n",
    "                # don't have offshore\n",
    "                pass\n",
    "\n",
    "        # Create a new MultiIndex and ppend aggregated data to the dataset\n",
    "        tuples = [(tech, 'DE', attribute, 'own calculation', 'own calculation')]\n",
    "        colnames = pd.MultiIndex.from_tuples(tuples, names=headers)\n",
    "        sum_col.columns = colnames\n",
    "        \n",
    "        \n",
    "        data_sets['15min'] = data_sets['15min'].combine_first(sum_col)\n",
    "\n",
    "        if not attribute == 'generation':\n",
    "            continue\n",
    "        \n",
    "        # Calculate the profile column            \n",
    "        profile_col = sum_col.values / data_sets['15min'][tech, 'DE', 'capacity']\n",
    "        \n",
    "        # Create a new MultiIndex and ppend aggregated data to the dataset\n",
    "        tuples = [(tech, 'DE', 'profile', 'own calculation', 'own calculation')]\n",
    "        columns = pd.MultiIndex.from_tuples(tuples, names=headers)\n",
    "        profile_col.columns = columns\n",
    "        data_sets['15min'] = data_sets['15min'].combine_first(profile_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Reorder Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided thatit is more intuitive to order the columns first by country, then by variable and finally attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    data_sets[res_key][('', 'comment','', '', '')] = df['comment']\n",
    "    data_sets[res_key].drop('comment', axis=1, inplace=True)\n",
    "    data_sets[res_key] = df.swaplevel(i='region', j='variable', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4e311f14-5124-47fd-9f94-0a866e9a6f71"
    }
   },
   "source": [
    "## 5.4 Create hourly data from 15' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "882bf163-9e93-44a4-9c51-a3ce0c32b4f4"
    }
   },
   "source": [
    "Some data comes in 15-minute intervals (i.e. German renewable generation), other in 60-minutes (i.e. load data from ENTSO-E and Prices). We resample the 15-minute data to hourly resolution and append it to the 60-minutes dataset.\n",
    "\n",
    "The marker column is resampled separately in such a way that all information on where data has been interpolated is preserved.\n",
    "\n",
    "The `.resample('H').mean()` methods calculates the means from the values for 4 quarter hours [:00, :15, :30, :45] of an hour values, inserts that for :00 and drops the other 3 entries. Takes 15 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def resample_markers(group):\n",
    "    '''Resample marker column from 15 to 60 min\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group: pd.Series\n",
    "        Series of 4 succeeding quarter-hourly values from the marker column\n",
    "        that have to be combined into one.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    aggregated_marker : str or np.nan\n",
    "        If there were any markers in group: the unique values from the marker\n",
    "        column group joined together in one string, np.nan otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if group.notnull().values.any():\n",
    "        unpacked = [mark for line in group if type(line) is str for mark in line.split(';')[:-1]]\n",
    "        aggregated_marker = '; '.join(set(unpacked)) + '; '\n",
    "    else:\n",
    "        aggregated_marker = np.nan\n",
    "    return aggregated_marker\n",
    "\n",
    "\n",
    "marker_col_15 = data_sets['15min']['comment']\n",
    "marker_col_15 = marker_col_15.groupby(\n",
    "    pd.Grouper(freq='60Min', closed='left', label='left')).agg(resample_markers)\n",
    "marker_col_15 = marker_col_15.reindex(data_sets['60min'].index)\n",
    "data_sets['60min']['comment'] = (\n",
    "    data_sets['60min']['comment']\n",
    "    .str.cat(others=marker_col_15, sep='', na_rep='')\n",
    "    .replace(to_replace='', value=np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c19e3eb9-feca-4e94-9035-580aa07189ba"
    }
   },
   "outputs": [],
   "source": [
    "%time\n",
    "resampled = data_sets['15min'].resample('H').mean()\n",
    "try:\n",
    "    data_sets['60min'] = data_sets['60min'].combine_first(resampled)\n",
    "except KeyError:\n",
    "    data_sets['60min'] = resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dea911fb-326f-46a6-a009-57af147d4be4"
    }
   },
   "source": [
    "## 5.5 Insert a column with Central European (Summer-)time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index column of th data sets defines the start of the timeperiod represented by each row of that data set in **UTC** time. We include an additional column for the **CE(S)T** Central European (Summer-) Time, as this might help aligning the output data with other data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_cols = {'utc': 'utc_timestamp',\n",
    "             'cet':'cet_cest_timestamp',\n",
    "             'marker': 'comment'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b1550779-53cc-498d-980b-7aa253974c91"
    }
   },
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df.index.rename(info_cols['utc'], inplace=True)\n",
    "    df.insert(0, info_cols['cet'],\n",
    "              df.index.tz_localize('UTC').tz_convert('Europe/Brussels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Reorder columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the additional timestamp and the marker column to the left of the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    cols = df.columns.tolist()\n",
    "    cols = [cols[-1]] + [cols[-2]]+ cols[:-2]\n",
    "    data_sets[res_key] = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4e625ea7-c448-45fe-85b2-026157ad24c0"
    }
   },
   "source": [
    "# 6. Create metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "38b6a454-31c4-4112-971f-da8271131d54"
    }
   },
   "source": [
    "This section: create the metadata, both general and column-specific. All metadata we be stored as a JSON file. Takes <1s to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "version = '2016-11-08'\n",
    "headers = ['region', 'variable', 'attribute', 'source', 'web']\n",
    "changes = '''\n",
    "included \\\"wind\\\" additionally to \\\"wind-onshore\\\" and \\\"wind-offshore\\\" columns\n",
    "for Germany and moved \\\"region\\\" to the top of the column index, so it now\n",
    "reads i.e. \\\"DE_wind_generation\\\" instead of \\\"wind_DE_generation\\\"'''\n",
    "\n",
    "make_json(data_sets, info_cols, version, changes, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fa919796-a7f6-4556-aeed-181ddc6028ac"
    }
   },
   "source": [
    "# 7. Write data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1b3c9199-ce14-4487-939d-656f60c14df3"
    }
   },
   "source": [
    "This section: Save as [Data Package](http://data.okfn.org/doc/tabular-data-package) (data in CSV, metadata in JSON file). All files are saved in the directory of this notebook. Alternative file formats (SQL, XLSX) are also exported. Takes about 1 hour to run.\n",
    "\n",
    "But first, create a final savepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min'].to_pickle('final_15.pickle')\n",
    "data_sets['60min'].to_pickle('final_60.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "data_sets['15min'] = pd.read_pickle('final_15.pickle')\n",
    "data_sets['60min'] = pd.read_pickle('final_60.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "47c1efa2-d93f-4d13-81d7-8f64dadeff3f"
    }
   },
   "source": [
    "## 7.1 Different shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a753ac43-a0f4-44bc-a89d-1ccaaf48289a"
    }
   },
   "source": [
    "Data are provided in three different \"shapes\": \n",
    "- SingleIndex (easy to read for humans, compatible with datapackage standard, small file size)\n",
    "  - Fileformat: CSV, SQLite\n",
    "- MultiIndex (easy to read into GAMS, not compatible with datapackage standard, small file size)\n",
    "  - Fileformat: CSV, Excel\n",
    "- Stacked (compatible with data package standard, large file size, many rows, too many for Excel) \n",
    "  - Fileformat: CSV\n",
    "\n",
    "The different shapes need to be created internally befor they can be saved to files. Takes about 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "769225c6-31f5-4db8-8d91-32a3f983489c"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\opsd_time_series\\lib\\site-packages\\ipykernel\\__main__.py:27: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_sets_singleindex = {}\n",
    "data_sets_multiindex = {}\n",
    "data_sets_stacked = {}\n",
    "for res_key, df in data_sets.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    for col_name, col in df.iteritems():\n",
    "        if not (col_name[0] in info_cols.values() or\n",
    "                col_name[2] == 'profile'):\n",
    "            df[col_name] = col.round(0)\n",
    "\n",
    "    # MultIndex\n",
    "    data_sets_multiindex[res_key + '_multiindex'] = df\n",
    "\n",
    "    # SingleIndex\n",
    "    df_singleindex = df.copy()\n",
    "    # use first 3 levels of multiindex to create singleindex\n",
    "    df_singleindex.columns = [\n",
    "        col[0] if col[0] in info_cols.values()\n",
    "        else '_'.join(col[0:3]) for col in df.columns.values]\n",
    "\n",
    "    data_sets_singleindex[res_key + '_singleindex'] = df_singleindex\n",
    "\n",
    "    # Stacked\n",
    "    stacked = df.copy()\n",
    "    stacked.drop(info_cols['cet'], axis=1, inplace=True)\n",
    "    stacked.columns = stacked.columns.droplevel(['source', 'web'])\n",
    "    stacked = stacked.transpose().stack(dropna=True).to_frame(name='data')\n",
    "    data_sets_stacked[res_key + '_stacked'] = stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "84f1822e-3aa6-42c4-a424-5dc5ab6fa56f"
    }
   },
   "source": [
    "## 7.2 Write to SQL-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3bb78fa9-5309-46b6-b945-68dcb654a567"
    }
   },
   "source": [
    "This file format is required for the filtering function on the OPSD website. This takes about 30 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mC:\\Miniconda3\\envs\\opsd_time_series\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_make_dt_accessor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2739\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_to_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2740\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Miniconda3\\envs\\opsd_time_series\\lib\\site-packages\\pandas\\tseries\\common.py\u001b[0m in \u001b[0;36mmaybe_to_datetimelike\u001b[0;34m(data, copy)\u001b[0m\n\u001b[1;32m     84\u001b[0m     raise TypeError(\"cannot convert an object of type {0} to a \"\n\u001b[0;32m---> 85\u001b[0;31m                     \"datetimelike index\".format(type(data)))\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert an object of type <class 'pandas.core.series.Series'> to a datetimelike index",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-ab9e936f6344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_sets_singleindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'15min_singleindex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minfo_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%dT%H:%M:%S%z'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Miniconda3\\envs\\opsd_time_series\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2738\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[1;32m   2739\u001b[0m                 name in self._accessors):\n\u001b[0;32m-> 2740\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Miniconda3\\envs\\opsd_time_series\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[1;31m# this ensures that Series.str.<method> is well defined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccessor_cls\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__set__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Miniconda3\\envs\\opsd_time_series\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_make_dt_accessor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2739\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_to_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2740\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2741\u001b[0;31m             raise AttributeError(\"Can only use .dt accessor with datetimelike \"\n\u001b[0m\u001b[1;32m   2742\u001b[0m                                  \"values\")\n\u001b[1;32m   2743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "data_sets_singleindex['15min_singleindex'][info_cols['cet']].dt.strftime('%Y-%m-%dT%H:%M:%S%z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "fd35212c-ec5c-4fcf-9897-4608742d1bf8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for res_key, df in data_sets_singleindex.items():\n",
    "    f = 'time_series_' + res_key\n",
    "    df = df.copy()\n",
    "    df.index = df.index.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "#    df[info_cols['cet']] = df[info_cols['cet']].dt.strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "#    df.to_sql(f, sqlite3.connect('time_series.sqlite'),\n",
    "#              if_exists='replace', index_label=info_cols['utc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "143b39aa-dd2e-4923-be56-bb6c4706837d"
    }
   },
   "source": [
    "## 7.3 Write to Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef7763f5-9bbc-40b8-8cee-829131b40336"
    }
   },
   "source": [
    "Writing the full tables to Excel takes extremely long. As a workaround, only the first 5 rows are exported. The rest of the data can than be inserted manually from the `_multindex.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9e84c62a-7bd6-4319-89dd-409574dda234"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "writer = pd.ExcelWriter('time_series.xlsx')\n",
    "for res_key, df in data_sets_multiindex.items():\n",
    "    df.head().to_excel(writer, res_key, float_format='%.2f', merge_cells=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "454ee5f5-e8f1-4088-94e9-e846f48ee75b"
    }
   },
   "source": [
    "## 7.4 Write to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "279e3306-6dea-454d-bec8-0c826509ecd1"
    }
   },
   "source": [
    "This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "89449c49-608d-488d-8bc8-077c64bc26c7"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# itertoools.chain() allows iterating over multiple dicts at once\n",
    "for res_stacking_key, df in itertools.chain(\n",
    "        data_sets_singleindex.items(),\n",
    "        data_sets_multiindex.items(),\n",
    "        data_sets_stacked.items()\n",
    "    ):\n",
    "    # convert the format of the cet_cest-timestamp to ISO-8601\n",
    "    if not (res_stacking_key in ['15min_stacked', '60min_stacked']\n",
    "            or type(df.iloc[0,0]) == str):\n",
    "        df.iloc[:,0] = df.iloc[:,0].dt.strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "    f = 'time_series_' + res_stacking_key\n",
    "    df.to_csv(f + '.csv', float_format='%.2f',\n",
    "              date_format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:opsd_time_series]",
   "language": "python",
   "name": "conda-env-opsd_time_series-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "f6b300bf-88b5-4dea-951e-c926a9ea8287": {
     "id": "f6b300bf-88b5-4dea-951e-c926a9ea8287",
     "prev": "f96dd4bc-93a6-4014-b85f-a43061cf5688",
     "regions": {
      "dc486e18-7547-4610-99c0-55dfb5553f62": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "c0035fc6-ff1d-44d8-a3fd-b4c08f53be71",
        "part": "source"
       },
       "id": "dc486e18-7547-4610-99c0-55dfb5553f62"
      }
     }
    },
    "f96dd4bc-93a6-4014-b85f-a43061cf5688": {
     "id": "f96dd4bc-93a6-4014-b85f-a43061cf5688",
     "prev": null,
     "regions": {
      "657c3ad3-2fcf-4c8e-a527-de3d0a46fa4e": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "1562965a-7d74-4c1c-8251-4d82847f294a",
        "part": "source"
       },
       "id": "657c3ad3-2fcf-4c8e-a527-de3d0a46fa4e"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

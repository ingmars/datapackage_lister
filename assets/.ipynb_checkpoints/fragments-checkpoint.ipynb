{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = {{a}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "cmd": "Other",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "latex": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "cls": "KernelMagics",
        "colors": "BasicMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "copy": "Other",
        "ddir": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "echo": "Other",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "install_default_config": "DeprecatedMagics",
        "install_ext": "ExtensionMagics",
        "install_profiles": "DeprecatedMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "profile": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "ren": "Other",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %install_default_config  %install_ext  %install_profiles  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%latex  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 7, 13, 13, 24, 36, 173586)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\n",
    "    os.path.join(\n",
    "        out_path, source_name, variable_name,\n",
    "        start.strftime('%Y-%m-%d') + '_' +\n",
    "        end.strftime('%Y-%m-%d')\n",
    "    ), exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        else:\n",
    "            # The files on the servers usually contain the data for subperiods\n",
    "            # of some regular length (i.e. months or yearsavailable\n",
    "            # Create lists of start- and enddates of periods represented in\n",
    "            # individual files to be downloaded.\n",
    "#            if source_name == 'Elia':\n",
    "#                timezone = 'Europe/Brussels'\n",
    "#            else:\n",
    "#                timezone = None \n",
    "            starts = pd.date_range(\n",
    "                start=param_dict['start'], end=param_dict['end'],\n",
    "                freq=param_dict['frequency'] + 'S',\n",
    "                #tz=timezone\n",
    "            )\n",
    "            ends = pd.date_range(\n",
    "                start=param_dict['start'], end=param_dict['end'],\n",
    "                freq=param_dict['frequency'],\n",
    "                #tz=timezone\n",
    "            )\n",
    "#            if source_name == 'Elia':\n",
    "#                starts = starts.tz_convert(None)\n",
    "#                ends = ends.tz_convert(None)\n",
    "            if len(ends) == 0:\n",
    "                ends = pd.DatetimeIndex([param_dict['end']])\n",
    "    \n",
    "            for s, e in zip(starts, ends):\n",
    "                download_file(\n",
    "                    source_name, variable_name, out_path, param_dict,\n",
    "                    start=s, end=e, session=session\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_elia(filepath, variable_name, web, headers):\n",
    "    \"\"\"\n",
    "    Read a .xls file with hourly load data from the Energinet DK Data Portal\n",
    "    into a dataframe. Returns a pandas.DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Directory path of file to be read.\n",
    "    variable_name : str\n",
    "        Name of variable, e.g. ``solar`\n",
    "    web : str\n",
    "        URL linking to the source website where this data comes from.\n",
    "    headers : list\n",
    "        List of strings indicating the level names of the pandas.MultiIndex\n",
    "        for the columns of the dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(\n",
    "        io=filepath,\n",
    "        header = None,\n",
    "        skiprows = 4,\n",
    "        index_col = 0,#'timestamp',\n",
    "        parse_cols = [0, 2, 4, 5]\n",
    "    )\n",
    "#    import pdb; pdb.set_trace() #this is a breakpoint\n",
    "#    df.reset_index(inplace=True)\n",
    "    df.columns = ['forecast', 'generation', 'capacity']#'timestamp', 'forecast', 'generation', 'capacity']\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index.rename('timestamp'))#['timestamp'])\n",
    "#    df.set_index('timestamp', inplace=True) \n",
    "    \n",
    "    df.index = df.index.tz_localize('Europe/Brussels', ambiguous='infer')\n",
    "    df.index = df.index.tz_convert(None)\n",
    "\n",
    "    # Create the MultiIndex\n",
    "    tuples = [\n",
    "        (variable_name, 'BE', attribute, 'Elia', web)\n",
    "        for attribute\n",
    "        in df.columns\n",
    "    ]\n",
    "    columns = pd.MultiIndex.from_tuples(tuples, names=headers)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://www.svk.se/siteassets/aktorsportalen/elmarknad/statistik/sverigestatistik/n_fot201001-06.xls\n",
    "http://www.svk.se/siteassets/aktorsportalen          /statistik/sverigestatistik/n_fot201001-06.xls\n",
    "http://www.svk.se/siteassets/aktorsportalen/elmarknad/statistik/sverigestatistik/forbrukning-och-tillforsel-per-timme--i-normaltid-2016.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.MultiIndex.from_arrays([data_sets['15min'].index, data_sets['15min']['cet_time']])\n",
    "#data_sets['15min'].index = pd.MultiIndex.from_arrays([data_sets['15min'].index, data_sets['15min'].index.tz_localize('UTC').tz_convert('Europe/Berlin')])\n",
    "#data_sets['15min'].set_index([data_sets['15min'].index, 'cet_time'], append=True)#, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolator(i, j, row, df, col_name, nan_regs, one_period):\n",
    "    '''interpolate missing value spans up to 2 hours'''\n",
    "    if i + 1 == len(nan_regs):\n",
    "        logger.info(\n",
    "            '%s : \\n         '\n",
    "            'interpolated %s up-to-2-hour-spans of NaNs',\n",
    "            col_name[0:3], i + 1 - j\n",
    "        )\n",
    "    to_fill = slice(row['start_idx'] - one_period, row['till_idx'] + one_period)\n",
    "    df.iloc[:,0].loc[to_fill] = df.iloc[:,0].loc[to_fill].interpolate()\n",
    "    return df\n",
    "\n",
    "def guesser(row, df, col_name, nan_regs, frame, one_period):\n",
    "    '''guess missing value spans longer than one hour based on other tsos'''\n",
    "    #logger.info('guessed %s entries after %s', row['count'], row['start_idx'])\n",
    "    day_before = pd.DatetimeIndex(\n",
    "        freq='15min',\n",
    "        start=row['start_idx'] - timedelta(hours=24),\n",
    "        end=row['start_idx'] - one_period\n",
    "    )\n",
    "\n",
    "    to_fill = pd.DatetimeIndex(\n",
    "        freq='15min',\n",
    "        start=row['start_idx'],\n",
    "        end=row['till_idx']\n",
    "    )\n",
    "\n",
    "    # other_tsos = [c[1] for c in compact.drop(col_name, axis=1).loc[:,(col_name[0],slice(None),col_name[2])].columns.tolist()]\n",
    "    other_tsos = [\n",
    "        tso\n",
    "        for tso\n",
    "        in ['DE50hertz', 'DEamprion', 'DEtennet', 'DEtransnetbw']\n",
    "        if tso != col_name[1]\n",
    "    ]\n",
    "\n",
    "    # select columns with data for same technology (wind/solar) but from other TSOs\n",
    "    similar = frame.loc[:,(col_name[0],other_tsos,col_name[2])]\n",
    "    # calculate the sum using columns without NaNs the day \n",
    "    # before or during the period to be guessed\n",
    "    similar = similar.dropna(\n",
    "        axis=1,\n",
    "        how='any',\n",
    "        subset=day_before.append(to_fill)\n",
    "    ).sum(axis=1)\n",
    "    # calculate scaling factor for other TSO data\n",
    "    factor =  similar.loc[day_before].sum(axis=0) / df.loc[day_before,:].sum(axis=0)\n",
    "\n",
    "    guess = similar.loc[to_fill] / float(factor)\n",
    "    df.iloc[:,0].loc[to_fill] = guess\n",
    "    a = float(df.iloc[:,0].loc[row['start_idx'] - one_period])\n",
    "    b = float(df.iloc[:,0].loc[row['start_idx']])\n",
    "    if a == 0:\n",
    "        deviation = '{} absolut'.format(a - b)\n",
    "    else:\n",
    "        deviation = '{:.2f} %'.format((a - b) / a * 100)\n",
    "    logger.info(\n",
    "        '%s : \\n        '\n",
    "        'guessed %s entries after %s \\n        '\n",
    "        'last non-missing: %s \\n        '\n",
    "        'first guessed: %s \\n        '\n",
    "        'deviation of first guess from last known value: %s',\n",
    "        col_name[0:3], row['count'], row['start_idx'], a, b, deviation\n",
    "    )                      \n",
    "    return df\n",
    "\n",
    "def chooser(df, col_name, nan_regs, frame, one_period):\n",
    "    for i, row in nan_regs.iterrows():\n",
    "        j = 0\n",
    "        # interpolate missing value spans up to 2 hours\n",
    "        if row['span'] <= timedelta(hours=2):\n",
    "            df = interpolator(i, j, row, df, col_name, nan_regs, one_period)\n",
    "        # guess missing value spans longer than one hour based on other tsos\n",
    "        elif col_name[1][:2] == 'DE' and col_name[2] == 'generation':\n",
    "            j += 1\n",
    "            df = guesser(row, df, col_name, nan_regs, frame, one_period)\n",
    "    return df\n",
    "\n",
    "def nan_finder(frame, patch=False):\n",
    "    '''Search for missing values in a DataFrame and apply custom patching.'''\n",
    "    nan_table = pd.DataFrame()\n",
    "    patched = pd.DataFrame()\n",
    "    one_period = frame.index[1] - frame.index[0]\n",
    "    for col_name, col in frame.iteritems():\n",
    "        df = col.to_frame() # kann man colname wieder an df drankleben? df sollte col heißen\n",
    "\n",
    "        # tag all occurences of NaN in the data (but not before first actual entry or after last one)\n",
    "        df['tag'] = (\n",
    "            (df.index >= df.first_valid_index()) &\n",
    "            (df.index <= df.last_valid_index()) &\n",
    "            df.isnull().transpose().as_matrix()\n",
    "        ).transpose()\n",
    "\n",
    "        # make another DF to hold info about each region\n",
    "        nan_regs = pd.DataFrame()\n",
    "\n",
    "        # first row of consecutive region is a True preceded by a False in tags\n",
    "        nan_regs['start_idx'] = df.index[\n",
    "            df['tag'] & ~ \n",
    "            df['tag'].shift(1).fillna(False)]\n",
    "\n",
    "        # last row of consecutive region is a False preceded by a True   \n",
    "        nan_regs['till_idx'] = df.index[\n",
    "            df['tag'] & ~ \n",
    "            df['tag'].shift(-1).fillna(False)] \n",
    "        \n",
    "        if not df['tag'].any():\n",
    "            logger.info('%s : nothing to patch in this column', col_name[0:3])\n",
    "            df.drop('tag', axis=1, inplace=True)\n",
    "            nan_idx = pd.MultiIndex.from_arrays([\n",
    "                    [0, 0, 0, 0],\n",
    "                    ['count', 'span', 'start_idx', 'till_idx']\n",
    "                ]\n",
    "            )\n",
    "            nan_list = pd.DataFrame(index=nan_idx, columns=df.columns)\n",
    "        else:\n",
    "            # how long is each region\n",
    "            nan_regs['span'] = nan_regs['till_idx'] - nan_regs['start_idx'] + one_period\n",
    "            nan_regs['count'] = (nan_regs['span'] / one_period)\n",
    "            # sort the info DF to put longest missing region on top\n",
    "            nan_regs = nan_regs.sort_values(\n",
    "                'count',\n",
    "                ascending=False\n",
    "            ).reset_index(drop=True)\n",
    "            \n",
    "            df.drop('tag', axis=1, inplace=True)\n",
    "            nan_list = nan_regs.stack().to_frame()\n",
    "            nan_list.columns = df.columns\n",
    "            \n",
    "            if patch:\n",
    "                df = chooser(df, col_name, nan_regs, frame, one_period)\n",
    "        if len(patched) == 0:\n",
    "            patched = df\n",
    "        else:\n",
    "            patched = patched.combine_first(df)\n",
    "\n",
    "        if len(nan_table) == 0:\n",
    "            nan_table = nan_list\n",
    "        else:\n",
    "            nan_table = nan_table.combine_first(nan_list)\n",
    "\n",
    "    nan_table.columns.names = headers\n",
    "    patched.columns.names = headers\n",
    "\n",
    "    return patched, nan_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some code fragments for developmente purposes:\n",
    "#import pdb; pdb.set_trace() #this is a breakpoint\n",
    "#%pdb\n",
    "#%debug\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/Open-Power-System-Data/%20datapackage_timeseries/blob/2016-07-14/main.ipynb\n",
    "https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/2016-07-14/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='''\n",
    "http://www.svk.se/siteassets/aktorsportalen/elmarknad/statistik/sverigestatistik/forbrukning-och-tillforsel-per-timme--i-normaltid-2016.xls\n",
    "'''\n",
    "path = urllib.parse.urlsplit(url).path\n",
    "filename = posixpath.basename(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_general = \"\"\"\n",
    "name: >-\n",
    "    opsd-timeseries\n",
    "title: >-\n",
    "    Time-series data: load, wind and solar, prices\n",
    "description: >-\n",
    "    This dataset contains timeseries data of wind and solar in-feed into the\n",
    "    grids of German Transmission System Operators (TSOs) as well as load\n",
    "    timeseries for 37 European countries from ENTSO-E.\n",
    "opsd-jupyter-notebook-url: 'https://github.com/Open-Power-System-Data/\\\n",
    "datapackage_timeseries/blob/master/main.ipynb'\n",
    "version: >-\n",
    "    2016-03-04\n",
    "opsd-changes-to-last-version: >-\n",
    "    Various things...\n",
    "keywords:\n",
    "  - timeseries\n",
    "  - electricity\n",
    "  - in-feed\n",
    "  - capacity\n",
    "  - renewables\n",
    "  - wind\n",
    "  - solar\n",
    "  - load\n",
    "  - tso\n",
    "  - europe\n",
    "  - germany\n",
    "geographical-scope: >-\n",
    "    Europe/Germany\n",
    "licenses: \n",
    "  - url: http://example.com/license/url/here\n",
    "    version: 1.0\n",
    "    name: License Name Here\n",
    "    id: license-id-from-open\n",
    "sources: \n",
    "  - \n",
    "maintainers:\n",
    "  - web: http://example.com/\n",
    "    name: Jonathan Muehlenpfordt\n",
    "    email: muehlenpfordt@neon-energie.de\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ordered_load(stream, Loader=yaml.Loader, object_pairs_hook=OrderedDict):\n",
    "    class OrderedLoader(Loader):\n",
    "        pass\n",
    "    def construct_mapping(loader, node):\n",
    "        loader.flatten_mapping(node)\n",
    "        return object_pairs_hook(loader.construct_pairs(node))\n",
    "    OrderedLoader.add_constructor(\n",
    "        yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n",
    "        construct_mapping)\n",
    "    return yaml.load(stream, OrderedLoader)\n",
    "# usage example:\n",
    "metadata_ordered = ordered_load(metadata, yaml.SafeLoader)\n",
    "datapackage_json_ordered = json.dumps(metadata_ordered, indent=2, separators=(',', ': '))\n",
    "with open('datapackage.json', 'w') as f:\n",
    "    f.write(datapackage_json_ordered)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "json_data=open('datapackage.json').read()\n",
    "data = yaml.dump(json.loads(json_data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Übersicht über missing Values\n",
    "\n",
    "for column in resultDataSet.columns:\n",
    "    for source, t in conf.items():\n",
    "        for tech, param in t.items():\n",
    "            if source in column and tech in column:\n",
    "\n",
    "DST-Umstellungstage\n",
    " dst_transition_days = [d.date() for d in pytz.timezone('Europe/Berlin')._utc_transition_times[1:]]\n",
    "\n",
    "data_set.update(dataToAdd)\n",
    "\n",
    "#data_set.drop('50hertz', axis=1, level='source', inplace=True, errors='raise')\n",
    "#data_set.xs('50hertz', level='source', axis=1, drop_level=False)\n",
    "\n",
    "data_to_add.columns = data_to_add.columns.set_levels([url], level=4)\n",
    "\n",
    "data_sets['15min']['wind', 'DEtennet', 'actual', 'tennet'] #['DE']['actual']\n",
    "\n",
    "data_sets['15min'].head()#[ : ]['DK']\n",
    "\n",
    "\n",
    "\n",
    "metadata_unordered = yaml.load(metadata)\n",
    "\n",
    "datapackage_json_unordered = json.dumps(metadata_unordered, indent=2, separators=(',', ': '))\n",
    "\n",
    "with open('datapackage_unordered.json', 'w') as f:\n",
    "    f.write(datapackage_json_unordered)\n",
    "\n",
    "\n",
    "\n",
    "        sum_col = pd.Series()\n",
    "        for source in ['50hertz', 'amprion', 'tennet', 'transnetbw']:\n",
    "            add_col = data_sets['15min'][tech, 'DE' + source, attribute]#, source]\n",
    "            if len(sum_col) == 0:\n",
    "                sum_col = add_col\n",
    "            else:\n",
    "                sum_col = sum_col + add_col.values\n",
    "\n",
    "        # It's weird, but it seems we have to set skipna=False\n",
    "        # sum_col = data_sets['15min'][tech, 'DE', attribute].sum(axis=1, skipna=False).to_frame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_sets_singleindex = copy.deepcopy(data_sets)\n",
    "for res_key, data_set in data_sets.items():\n",
    "    columns_singleindex = []\n",
    "    for col in data_set.columns:\n",
    "        h = {k: v for k, v in zip(COLUMN_HEADERS, col)}\n",
    "        if h['source'] in ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW']:\n",
    "            h['country'] = h['country'] + h['source'].lower()            \n",
    "        col_singleindex = h['variable'] + '_' + h['country'] + '_' + h['attribute']\n",
    "        columns_singleindex.append(col_singleindex)\n",
    "    data_sets_singleindex[res_key].columns = columns_singleindex\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #if fill_gaps:\n",
    "            for i, row in regs_isnull.iterrows():\n",
    "                if row['spans'] <= timedelta(minutes=75):\n",
    "                    slicer = slice(row['start_idx'] - timedelta(minutes=15),\n",
    "                                   row['end_idx'] + timedelta(minutes=15))\n",
    "                    df.iloc[:,0].loc[slicer,] = df.iloc[:,0].loc[slicer,].interpolate()\n",
    "                similar = frame.loc[,()\n",
    "                    data_sets['15min'][tech, 'DE', attribute].sum(axis=1, skipna=False).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes time series data from a number of European *countries*, some of which are not actual countries but bidding zones or balancing areas. It includes *variables* which might come with *attributes*. A few examples:\n",
    "* **Countries**: DE, FR, UK, SE, SE1, SE2, DK, DK1, DEtennet\n",
    "* **Variables**: wind, solar, load, price\n",
    "* **Attributes**: generation, forecast, capacity, profile\n",
    "\n",
    "Not all combinations make sense or are available. For example, the attributes generation, forecast, capacity, profile apply to the variables wind and solar only (not to price or load). At this stage, we provide wind and solar data for Germany only, but load data for more than 30 countries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following lists the data sources used for this (time series) script. For an overview of power system data sources more generally see our [Data Sources](http://open-power-system-data.org/data-sources) project page. \n",
    "\n",
    "**Load data**\n",
    "- [ENTSO-E](https://www.entsoe.eu/data/data-portal/consumption/Pages/default.aspx)\n",
    "\n",
    "**Wind and solar generation data**\n",
    "- [50Hertz](http://www.50hertz.com/en/Grid-Data)\n",
    "- [Amprion](http://www.amprion.net/en/grid-data)\n",
    "- [TransnetBW](https://www.transnetbw.com/en/key-figures)\n",
    "- [TenneT](http://www.tennettso.de/site/en/Transparency/publications/network-figures/overview)\n",
    "\n",
    "**Wind and solar capacity data**\n",
    "- ...\n",
    "\n",
    "**Spot price data**\n",
    "- [Energinet.dk](http://www.energinet.dk/en/el/engrosmarked/udtraek-af-markedsdata/Sider/default.aspx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This is a python script that downloads and processes time-series data from European power systems. Data include electricity consumption (load) from ENTSO-E, wind and solar power generation from various transmission system operators, and wind and solar capacity data from different sources. Processing and data sources are documented in this notebook.\n",
    "\n",
    "The script produces two CSV files containing all data: one on hourly resolution and one in quarter-hourly resolution. The latter containts only those parameters that are available in quarter-hourly resolution.\n",
    "\n",
    "Download all data usually takes several hours. The two output files are about 50 MB each.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each Column in the dataFrame that we are going to create can be described by the following 4 criteria:\n",
    "* variable\n",
    "* country\n",
    "* attribute\n",
    "* source\n",
    "\n",
    "for example:\n",
    "* wind\n",
    "* DE\n",
    "* generation\n",
    "* transnetbw\n",
    "\n",
    "| variable | wind |\n",
    "| :--- | :--- |\n",
    "| country | DE |\n",
    "| attribute | generation |\n",
    "| source | transnetbw |\n",
    "\n",
    "This information will stored in a MultiIndex with 5 levels. Here we define the names of the MultiIndex levels.\n",
    "\n",
    "\n",
    "## 3.2 Data documentation and interpretation\n",
    "Often, the data that we use is poorly documented. In some cases, primary data owners provide some documentation.\n",
    "\n",
    "\n",
    "**Load data**\n",
    "* [ENTSO-E Specific national considerations](https://www.entsoe.eu/Documents/Publications/Statistics/Specific_national_considerations.pdf)\n",
    "* [Schumacher & Hirth 2015](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2715986), a paper on load data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

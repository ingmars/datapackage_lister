{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the project [Open Power System Data](http://open-power-system-data.org/).\n",
    "\n",
    "Find the latest version of this notebook an [GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/download.read)\n",
    "\n",
    "Go back to the main notebook ([GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/main.ipynb?) / [local copy](main.ipynb))\n",
    "\n",
    "This notebook reads the data saved by the download script ([GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/download.ipynb) / [local copy](download.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Read](#1.-Read)\n",
    "* [2. Preparations](#2.-Preparations)\n",
    "\t* [2.1 Libraries](#2.1-Libraries)\n",
    "\t* [2.2 Set up a log.](#2.2-Set-up-a-log.)\n",
    "\t* [2.3 Locate the download directory](#2.3-Locate-the-download-directory)\n",
    "\t* [2.4 Set the level names of the MultiIndex](#2.4-Set-the-level-names-of-the-MultiIndex)\n",
    "* [3. read-functions for individual data sources](#3.-read-functions-for-individual-data-sources)\n",
    "\t* [3.1 ENTSO-E](#3.1-ENTSO-E)\n",
    "\t* [3.2 '50Hertz](#3.2-'50Hertz)\n",
    "\t* [3.3 Amprion](#3.3-Amprion)\n",
    "\t* [3.4 TenneT](#3.4-TenneT)\n",
    "\t* [3.5 TransnetBW](#3.5-TransnetBW)\n",
    "\t* [3.6 Capacities](#3.6-Capacities)\n",
    "* [4. Read files one by one](#4.-Read-files-one-by-one)\n",
    "\t* [4.1 Create empty DataFrames](#4.1-Create-empty-DataFrames)\n",
    "\t* [4.2 Apply the processing function one-by-one](#4.2-Apply-the-processing-function-one-by-one)\n",
    "* [5. Write the data to disk for further processing](#5.-Write-the-data-to-disk-for-further-processing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import pytz\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set up a log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('log')\n",
    "logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Locate the download directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the local path where the input data is stored. This script expects a file structure acording to the following schema:\n",
    "* \\working_directory\\downloadpath\\source\\resource\\container\\file.csv\n",
    "\n",
    "for example:\n",
    "* \\datapackage_timeseries\\original_data\\TransnetBW\\wind\\2010-01-01_2010-01-31\\mwindeinsp_ist_prognose_2010_01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloadpath = 'original_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Set the level names of the MultiIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the rows at the top of the data used to store metadata internally. In the output data created by the processing script ([local copy](processing.ipynb#) / [GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/processing.ipynb)), this information will be moved to the [datapackage.json](datapackage.json#) File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADERS = ['variable', 'country', 'attribute', 'source', 'web']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. read-functions for individual data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 ENTSO-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_entso(filepath, web):\n",
    "    df = pd.read_excel(\n",
    "        io=filepath,\n",
    "        header=9,\n",
    "        skiprows=None,\n",
    "        index_col=[0, 1], # create MultiIndex from first 2 columns ['Country', 'Day']\n",
    "        parse_cols = None # None means: parse all columns\n",
    "        )\n",
    "    \n",
    "    # Create a list of daylight savings transitions \n",
    "    dst_transitions = [d.replace(hour=2) for d in pytz.timezone(\n",
    "        'Europe/Berlin')._utc_transition_times[1:]]\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    df.columns.names = ['raw_hour']\n",
    "    \n",
    "    # The original data has days and countries in the rows and hours in the\n",
    "    # columns.  This rearranges the table, mapping hours on the rows and\n",
    "    # countries on the columns.  \n",
    "    df = df.stack(level='raw_hour').unstack(level='Country').reset_index()    \n",
    "    \n",
    "    # Truncate the hours column after 2 characters and replace letters \n",
    "    # which are there to indicate the order during fall dst-transition.      \n",
    "    df['hour'] = df['raw_hour'].str[:2].str.replace('A','').str.replace('B','')\n",
    "    # Hours are indexed 1-24 by ENTSO-E, but pandas requires 0-23, so we deduct 1.\n",
    "    df['hour'] = (df['hour'].astype(int) - 1).astype(str)\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['Day']+' '+df['hour']+':00')\n",
    "    df.set_index('timestamp', inplace=True)    \n",
    "    \n",
    "    # Drop 2nd occurence of 03:00 appearing in October data except for autumn\n",
    "    # dst-transition.  \n",
    "    df = df[~((df['raw_hour'] == '3B:00:00') & ~ (df.index.isin(dst_transitions)))]\n",
    "    \n",
    "    # Drop 03:00 for (spring) dst-transition. October data is unaffected because\n",
    "    # the format is 3A:00/3B:00.  \n",
    "    df = df[~((df['raw_hour'] == '03:00:00') & (df.index.isin(dst_transitions)))]\n",
    "    \n",
    "    df.drop(['Day', 'hour', 'raw_hour'], axis=1, inplace=True)\n",
    "    df.index = df.index.tz_localize('Europe/Brussels', ambiguous='infer')\n",
    "    df.index = df.index.tz_convert(None)\n",
    "    \n",
    "    df.rename(columns={'DK_W': 'DKw', 'UA_W': 'UAw'}, inplace=True)\n",
    "    \n",
    "    # replace strings indicating missing data with proper NaN-format.  \n",
    "    df = df.replace(to_replace='n.a.', value=np.nan)\n",
    "    \n",
    "    # Create the MultiIndex.  \n",
    "    tuples = [('load', country, 'load', 'ENTSO-E', web) for country in df.columns]\n",
    "    columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 '50Hertz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_hertz(filepath, tech_attribute, web):\n",
    "    tech = tech_attribute.split('_')[0]\n",
    "    attribute = tech_attribute.split('_')[1]\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=';',\n",
    "        header=3,\n",
    "        index_col='timestamp',\n",
    "        names=['date',\n",
    "               'time',\n",
    "               attribute],\n",
    "        parse_dates={'timestamp': ['date', 'time']},\n",
    "        date_parser=None,\n",
    "        dayfirst=True,\n",
    "        decimal=',',\n",
    "        thousands='.',\n",
    "        # truncate values in 'time' column after 5th character\n",
    "        converters={'time': lambda x: x[:5]},\n",
    "        usecols=[0, 1, 3],\n",
    "    )\n",
    "    \n",
    "    # Until 2006 as well as  in 2015, during the fall dst-transistion, only the \n",
    "    # wintertime hour (marked by a B in the data) is reported, the summertime \n",
    "    # hour, (marked by an A) is missing in the data.  \n",
    "    # dst_arr is a boolean array consisting only of \"False\" entries, telling \n",
    "    # python to treat the hour from 2:00 to 2:59 as wintertime.\n",
    "    if pd.to_datetime(df.index.values[0]).year not in range(2007,2015):\n",
    "        dst_arr = np.zeros(len(df.index), dtype=bool)\n",
    "        df.index = df.index.tz_localize('Europe/Berlin', ambiguous=dst_arr)\n",
    "    else:\n",
    "        df.index = df.index.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "    df.index = df.index.tz_convert(None)\n",
    "    \n",
    "    # Create the MultiIndex\n",
    "    tuples = [(tech, 'DE50hertz', attribute, '50Hertz', web)]\n",
    "    columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Amprion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_amprion(filepath, tech, web):\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=';',\n",
    "        header=0,\n",
    "        index_col='timestamp',\n",
    "        names=['date',\n",
    "               'time',\n",
    "               'forecast',\n",
    "               'generation'],\n",
    "        parse_dates={'timestamp' : ['date', 'time']},\n",
    "        date_parser=None,\n",
    "        dayfirst=True,\n",
    "        decimal=',',\n",
    "        thousands=None,\n",
    "        # Truncate values in 'time' column after 5th character.\n",
    "        converters={'time': lambda x: x[:5]},\n",
    "        usecols=[0, 1, 2, 3],        \n",
    "    )\n",
    "\n",
    "    index1 = df.index[df.index.year <= 2009]\n",
    "    index1 = index1.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "    \n",
    "    # In the years after 2009, during the fall dst-transistion, only the\n",
    "    # summertime hour is reported, the wintertime hour is missing in the data.  \n",
    "    # dst_arr is a boolean array consisting only of \"True\" entries, telling \n",
    "    # python to treat the hour from 2:00 to 2:59 as summertime.\n",
    "    index2 = df.index[df.index.year > 2009]\n",
    "    dst_arr = np.ones(len(index2), dtype=bool)\n",
    "    index2 = index2.tz_localize('Europe/Berlin', ambiguous=dst_arr)        \n",
    "    df.index = index1.append(index2)\n",
    "    df.index = df.index.tz_convert(None)\n",
    "    \n",
    "    # Create the MultiIndex\n",
    "    tuples = [(tech, 'DEamprion', attribute, 'Amprion', web) for attribute in df.columns]\n",
    "    columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "    df.columns = columns    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 TenneT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tennet Data doesn't feature a time column. Instead, the quarter-hourly data entries for each day are numbered by their position, creating an index ranging...\n",
    "* from 1 to 96 on normal days,\n",
    "* from 1 to 92 on spring dst-transition dates,\n",
    "* from 1 to 100 on fall dst-transition days.\n",
    "\n",
    "This index can be used to compute a timestamp. However, there are a couple of errors in the data, which is why a lot of exceptions need to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tennet(filepath, tech, web):\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=';',\n",
    "        encoding='latin_1',\n",
    "        header=3,\n",
    "        index_col=None,\n",
    "        names=['date',\n",
    "               'pos',\n",
    "               'forecast',\n",
    "               'generation'],\n",
    "        parse_dates=False,\n",
    "        date_parser=None,\n",
    "        dayfirst=True,\n",
    "        thousands=None,\n",
    "        converters=None,          \n",
    "        usecols=[0, 1, 2, 3],\n",
    "    )\n",
    "\n",
    "    df['date'].fillna(method='ffill', limit = 100, inplace=True)\n",
    "\n",
    "    for i in range(len(df.index)):\n",
    "        # On the day in March when summertime begins, shift the data forward by\n",
    "        # 1 hour, beginning with the 9th quarter-hour, so the index runs again\n",
    "        # up to 96\n",
    "        if (df['pos'][i] == 92 and\n",
    "            ((i == len(df.index)-1) or (df['pos'][i + 1] == 1))):\n",
    "            slicer = df[(df['date'] == df['date'][i]) & (df['pos'] >= 9)].index\n",
    "            df.loc[slicer, 'pos'] = df['pos'] + 4\n",
    "\n",
    "        if df['pos'][i] > 96: # True when summertime ends in October\n",
    "            logger.info('%s th quarter-hour at %s, position %s',\n",
    "                        df['pos'][i], df.ix[i,'date'], (i))  \n",
    "\n",
    "            # Instead of having the quarter-hours' index run up to 100, we want \n",
    "            # to have it set back by 1 hour beginning from the 13th\n",
    "            # quarter-hour, ending at 96\n",
    "            if (df['pos'][i] == 100 and not (df['pos'] == 101).any()):                    \n",
    "                slicer = df[(df['date'] == df['date'][i]) & (df['pos'] >= 13)].index\n",
    "                df.loc[slicer, 'pos'] = df['pos'] - 4                     \n",
    "\n",
    "            # In 2011 and 2012, there are 101 qaurter hours on the day the \n",
    "            # summertime ends, so 1 too many.  From looking at the data, we\n",
    "            # inferred that the 13'th quarter hour is the culprit, so we drop\n",
    "            # that.  The following entries for that day need to be shifted.\n",
    "            elif df['pos'][i] == 101: \n",
    "                df = df[~((df['date'] == df['date'][i]) & (df['pos'] == 13))]\n",
    "                slicer = df[(df['date'] == df['date'][i]) & (df['pos'] >= 13)].index\n",
    "                df.loc[slicer, 'pos'] = df['pos'] - 5     \n",
    "\n",
    "    # On 2012-03-25, there are 94 entries, where entries 8 and 10 are probably\n",
    "    # wrong.\n",
    "    if df['date'][0] == '2012-03-01':\n",
    "        df = df[~((df['date'] == '2012-03-25') & \n",
    "                  ((df['pos'] == 8) | (df['pos'] == 10)))]\n",
    "        slicer = df[(df['date'] == '2012-03-25') & (df['pos'] >= 9)].index\n",
    "        df.loc[slicer, 'pos'] = [8] + list(range(13, 97))        \n",
    "\n",
    "    # On 2012-09-27, there are 97 entries.  Probably, just the 97th entry is wrong.\n",
    "    if df['date'][0] == '2012-09-01':\n",
    "        df = df[~((df['date'] == '2012-09-27') & (df['pos'] == 97))]          \n",
    "\n",
    "    # Here we compute the timestamp from the position and generate the\n",
    "    # datetime-index\n",
    "    df['hour'] = (np.trunc((df['pos']-1)/4)).astype(int).astype(str)\n",
    "    df['minute'] = (((df['pos']-1)%4)*15).astype(int).astype(str)\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + ' ' + df['hour'] + ':' +\n",
    "                                     df['minute'], dayfirst = True)\n",
    "    df.set_index('timestamp',inplace=True)\n",
    "\n",
    "    # In the years 2006, 2008, and 2009, the dst-transition hour in March\n",
    "    # appears as empty rows in the data.  We delete it from the set in order to\n",
    "    # make the timezone localization work.  \n",
    "    for crucial_date in pd.to_datetime(['2006-03-26', '2008-03-30',\n",
    "                                        '2009-03-29']).date:\n",
    "        if df.index[0].year == crucial_date.year:\n",
    "            df = df[~((df.index.date == crucial_date) &\n",
    "                          (df.index.hour == 2))]\n",
    "\n",
    "    df.drop(['pos', 'date', 'hour', 'minute'], axis=1, inplace=True)\n",
    "\n",
    "    df.index = df.index.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "    df.index = df.index.tz_convert(None)\n",
    "    \n",
    "    # Create the MultiIndex\n",
    "    tuples = [(tech, 'DEtennet', attribute, 'TenneT', web) for attribute in df.columns]\n",
    "    columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 TransnetBW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_transnetbw(filepath, tech, web):\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=';',\n",
    "        header=0,\n",
    "        index_col='timestamp',\n",
    "        names=['date',\n",
    "               'time',\n",
    "               'forecast',\n",
    "               'generation'],\n",
    "        parse_dates={'timestamp': ['date', 'time']},\n",
    "        date_parser=None,         \n",
    "        dayfirst=True,\n",
    "        decimal=',',\n",
    "        thousands=None,\n",
    "        converters=None,\n",
    "        usecols=[2, 3, 4, 5],\n",
    "    )\n",
    "    \n",
    "    # 'ambigous' refers to how the October dst-transition hour is handled.  \n",
    "    # ‘infer’ will attempt to infer dst-transition hours based on order.\n",
    "    df.index = df.index.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "    df.index = df.index.tz_convert(None)\n",
    "    \n",
    "    # The time taken from column 3 indicates the end of the respective period.\n",
    "    # to construct the index, however, we need the beginning, so we shift the \n",
    "    # data back by 1 period.  \n",
    "    df = df.shift(periods=-1, freq='15min', axis='index')\n",
    "    \n",
    "    # Create the MultiIndex\n",
    "    tuples = [(tech, 'DEtransnetbw', attribute, 'TransnetBW', web) for attribute in df.columns]\n",
    "    columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_capacities(filepath, web):\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=',',\n",
    "        header=0,\n",
    "        index_col='timestamp',\n",
    "        names=['timestamp',\n",
    "               'wind',\n",
    "               'solar'],\n",
    "        parse_dates=True,\n",
    "        date_parser=None,         \n",
    "        dayfirst=True,\n",
    "        decimal='.',\n",
    "        thousands=None,\n",
    "        converters=None,\n",
    "        usecols=[0,2,3],\n",
    "    )\n",
    "    \n",
    "    last = pd.to_datetime([df.index[-1].replace(hour=23, minute=59)])\n",
    "    until_last = df.index.append(last).rename('timestamp')\n",
    "    df = df.reindex(index=until_last, method='ffill')\n",
    "    df.index = df.index.tz_localize('Europe/Berlin')\n",
    "    df.index = df.index.tz_convert(None)\n",
    "    df = df.resample('15min').ffill()\n",
    "    \n",
    "    # Create the MultiIndex\n",
    "    source = 'own calculation'\n",
    "    tuples = [(tech, 'DE', 'capacity', source, web) for tech in df.columns]\n",
    "    columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "    df.columns = columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Read files one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create empty DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary with an empty DataFrame each for data with 15/60 minute resolution. This line deletes all data previously loaded into the data_sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = {'15min': pd.DataFrame(), '60min': pd.DataFrame()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Apply the processing function one-by-one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each source/TSO and technology specified in the conf dict, this section finds all the downloaded files in the downloads folder and then calls the matching read function.\n",
    "The datasets returned by the read function are then merged with the other data of the same resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains a python dictionary indicating which datasources there are, which data types they provide and a link to the source to be included in the columnd header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = \"\"\"\n",
    "60min:\n",
    "    ENTSO-E:\n",
    "        load: https://www.entsoe.eu/data/data-portal/consumption/Pages/default.aspx #Hourly load values of all countries for a specific month\n",
    "15min:\n",
    "    50Hertz: \n",
    "        wind_generation: http://www.50hertz.com/en/Grid-Data/Wind-power/Archive-Wind-power\n",
    "        wind_forecast: http://www.50hertz.com/en/Grid-Data/Wind-power/Archive-Wind-power\n",
    "        solar_generation: http://www.50hertz.com/en/Grid-Data/Photovoltaics/Archive-Photovoltaics\n",
    "        solar_forecast: http://www.50hertz.com/en/Grid-Data/Photovoltaics/Archive-Photovoltaics\n",
    "    Amprion:\n",
    "        wind: http://www.amprion.net/en/wind-feed-in\n",
    "        solar: http://www.amprion.net/en/photovoltaic-infeed\n",
    "    TenneT:\n",
    "        wind: http://www.tennettso.de/site/en/Transparency/publications/network-figures/actual-and-forecast-wind-energy-feed-in\n",
    "        solar: http://www.tennettso.de/site/en/Transparency/publications/network-figures/actual-and-forecast-photovoltaic-energy-feed-in\n",
    "    TransnetBW:\n",
    "        wind: https://www.transnetbw.com/en/key-figures/renewable-energies/wind-infeed\n",
    "        solar: https://www.transnetbw.com/en/key-figures/renewable-energies/photovoltaic\n",
    "    OPSD:\n",
    "        capacities: http://data.open-power-system-data.org/datapackage_renewables/\n",
    "\"\"\"\n",
    "conf = yaml.load(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for resolution, sources in conf.items():\n",
    "    for source, resources in sources.items():\n",
    "        for resource, web in resources.items():\n",
    "            resource_dir = os.path.join(downloadpath, source, resource)\n",
    "            if not os.path.exists(resource_dir):\n",
    "                logger.info('folder not found for %s, %s', source, resource)\n",
    "            else:\n",
    "                for container in os.listdir(resource_dir):\n",
    "                    files = os.listdir(os.path.join(resource_dir, container))\n",
    "                    if not len(files) == 1:\n",
    "                        logger.info('error: found more than one file in %s %s %s',\n",
    "                                    source, resource, container)\n",
    "                    else:                        \n",
    "                        logger.info('reading %s %s %s',\n",
    "                                    source, resource, files[0])\n",
    "                        filepath = os.path.join(resource_dir, container, files[0])\n",
    "                        if os.path.getsize(filepath) < 128:\n",
    "                            logger.info('file is smaller than 128 Byte,' +\n",
    "                                    'which means it is probably empty')\n",
    "                        else:\n",
    "                            if source == 'ENTSO-E':\n",
    "                                data_to_add = read_entso(filepath, web)\n",
    "                            elif source == 'Svenska_Kraftnaet':\n",
    "                                data_to_add = read_svenskakraftnaet(filepath, source, resource, web)\n",
    "                            elif source == '50Hertz':\n",
    "                                data_to_add = read_hertz(filepath, resource, web)\n",
    "                            elif source == 'Amprion':\n",
    "                                data_to_add = read_amprion(filepath, resource, web)\n",
    "                            elif source == 'TenneT':\n",
    "                                data_to_add = read_tennet(filepath, resource, web)\n",
    "                            elif source == 'TransnetBW':\n",
    "                                data_to_add = read_transnetbw(filepath, resource, web)\n",
    "                            elif source == 'OPSD':\n",
    "                                data_to_add = read_capacities(filepath, web)\n",
    "                            \n",
    "                            # cut off data_to_add at end of year:\n",
    "                                data_to_add = data_to_add[:'2015-12-31 22:45:00']\n",
    "\n",
    "                            if len(data_sets[resolution]) == 0:\n",
    "                                data_sets[resolution] = data_to_add\n",
    "                            else:\n",
    "                                data_sets[resolution] = \\\n",
    "                                data_sets[resolution].combine_first(data_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Write the data to disk for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for resolution, data_set in data_sets.items():\n",
    "    data_set.to_csv('raw_data_' + resolution + '.csv', float_format='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should now be processed further. using the processing script ([GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/processing.ipynb) / [local copy](processing.ipynb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
